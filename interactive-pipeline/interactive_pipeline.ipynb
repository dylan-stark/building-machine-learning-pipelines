{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard_plugin_fairness_indicators==0.0.5 \\\n",
    "  tensorflow==2.2.0 \\\n",
    "  tensorflow_hub==0.8.0 \\\n",
    "  tensorflow_privacy==0.3.0 \\\n",
    "  tfx==0.22.0 \\\n",
    "  witwidget==1.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "from tfx.components import CsvExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = InteractiveContext(pipeline_root='../tfx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "data_dir = \"../data\"\n",
    "examples = external_input(os.path.join(base_dir, data_dir))\n",
    "example_gen = CsvExampleGen(input=examples)\n",
    "\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import StatisticsGen\n",
    "\n",
    "statistics_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import SchemaGen\n",
    "\n",
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=True)\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import ExampleValidator\n",
    "\n",
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "context.run(example_validator)\n",
    "\n",
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_file = os.path.join(base_dir, '../components/module.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tfx.components import Transform\n",
    "\n",
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=transform_file)\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_file = os.path.join(base_dir, '../components/module.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tfx.components import Trainer\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "\n",
    "TRAINING_STEPS = 1000\n",
    "EVALUATION_STEPS = 100\n",
    "\n",
    "trainer = Trainer(\n",
    "    module_file=trainer_file,\n",
    "    custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=TRAINING_STEPS),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=EVALUATION_STEPS))\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_artifact_dir = trainer.outputs['model'].get()[0].uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = os.path.join(model_artifact_dir, 'logs/')\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import ResolverNode\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model\n",
    "from tfx.types.standard_artifacts import ModelBlessing\n",
    "\n",
    "\n",
    "model_resolver = ResolverNode(\n",
    "      instance_name='latest_blessed_model_resolver',\n",
    "      resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
    "      model=Channel(type=Model),\n",
    "      model_blessing=Channel(type=ModelBlessing))\n",
    "context.run(model_resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb it always blesses on first run even if below threshold\n",
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "eval_config=tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "    slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "    metrics_specs=[\n",
    "          tfma.MetricsSpec(metrics=[\n",
    "              tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "              tfma.MetricConfig(class_name='ExampleCount'),\n",
    "              tfma.MetricConfig(class_name='AUC')\n",
    "              ],\n",
    "              thresholds={\n",
    "                  'AUC':\n",
    "                      tfma.config.MetricThreshold(\n",
    "                          value_threshold=tfma.GenericValueThreshold(\n",
    "                              lower_bound={'value': 0.65}),\n",
    "                          change_threshold=tfma.GenericChangeThreshold(\n",
    "                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                              absolute={'value': 0.01}))}\n",
    "                          )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tfx.components import Evaluator\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'],\n",
    "    baseline_model=model_resolver.outputs['model'],\n",
    "    eval_config=eval_config)\n",
    "context.run(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB TFMA visualizations will not run in Jupyter Lab\n",
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "# Get the TFMA output result path and load the result.\n",
    "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\n",
    "tfma_result = tfma.load_eval_result(PATH_TO_RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(tfma_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.proto import pusher_pb2\n",
    "\n",
    "_serving_model_dir = \"./tfx-9Apr/serving_model_dir\"\n",
    "\n",
    "pusher = Pusher(\n",
    "    model=trainer.outputs['model'],\n",
    "    model_blessing=evaluator.outputs['blessing'],\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory=_serving_model_dir)))\n",
    "context.run(pusher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../tfx-9Apr/serving_model_dir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "  \n",
    "!jupyter nbextension install --py --symlink tensorflow_model_analysis --sys-prefix\n",
    "  \n",
    "!jupyter nbextension enable --py tensorflow_model_analysis --sys-prefix\n",
    "\n",
    "# THEN REFRESH BROWSER PAGE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\n",
    "print(tfma.load_validation_result(PATH_TO_RESULT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data sliced by product\n",
    "tfma.view.render_slicing_metrics(\n",
    "    tfma_result, slicing_column='product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairness indicators direct from pipeline\n",
    "# https://colab.research.google.com/github/tensorflow/fairness-indicators/blob/master/fairness_indicators/examples/Fairness_Indicators_Lineage_Case_Study.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
